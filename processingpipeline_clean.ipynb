{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e46c84",
   "metadata": {},
   "source": [
    "# Urban Heat Island (UHI) Mapping Pipeline\n",
    "This notebook demonstrates the process of downloading, processing, and analyzing MODIS LST data for Urban Heat Island studies.\n",
    "## Table of Contents\n",
    "1. Setup & Configuration\n",
    "2. Download MODIS Data\n",
    "3. Extract LST Subdataset\n",
    "4. Convert Kelvin to Celsius\n",
    "5. Create Monthly Mean Composites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d376fc",
   "metadata": {},
   "source": [
    "This notebook has automated the process of detection of Urban Heat Islands in South Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f72c4d-7549-4b86-9478-3c8c176f3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Your NASA Earthdata credentials\n",
    "username = 'faatimaaamir2003'\n",
    "password = '9W.q364sXu2+GEw'\n",
    "\n",
    "# MODIS tiles covering Pakistan\n",
    "tiles = [\"h23v06\", \"h24v05\", \"h24v06\", \"h25v05\", \"h25v06\", \"h23v07\"]\n",
    "\n",
    "# Time range\n",
    "start_year = 2000\n",
    "end_year = 2025\n",
    "months = [5, 6, 7, 8]  # May to August\n",
    "days = [1, 9, 17, 25]  # MOD11A2 is an 8-day composite\n",
    "\n",
    "# Base URL\n",
    "base_url = 'https://e4ftl01.cr.usgs.gov/MOLT/MOD11A2.061/'\n",
    "\n",
    "# Create session for authentication\n",
    "session = requests.Session()\n",
    "session.auth = (username, password)\n",
    "\n",
    "# Optional: set download directory\n",
    "os.makedirs(\"MODIS_MOD11A2\", exist_ok=True)\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for month in months:\n",
    "        for day in days:\n",
    "            try:\n",
    "                dt = datetime(year, month, day)\n",
    "                doy = dt.timetuple().tm_yday\n",
    "                date_str = dt.strftime('%Y.%m.%d')\n",
    "                folder_url = f\"{base_url}{date_str}/\"\n",
    "\n",
    "                # Get directory listing\n",
    "                print(f\"Accessing {folder_url}\")\n",
    "                resp = session.get(folder_url)\n",
    "                if resp.status_code != 200:\n",
    "                    print(f\"Skipping {date_str}: HTTP {resp.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                links = soup.find_all('a')\n",
    "\n",
    "                for link in links:\n",
    "                    href = link.get('href')\n",
    "                    if not href:\n",
    "                        continue\n",
    "                    for tile in tiles:\n",
    "                        if href.startswith(f\"MOD11A2.A{year}{doy:03d}.{tile}.061\") and href.endswith(\".hdf\"):\n",
    "                            file_url = folder_url + href\n",
    "                            out_path = os.path.join(\"MODIS_MOD11A2\", href)\n",
    "                            if os.path.exists(out_path):\n",
    "                                print(f\"Already downloaded: {href}\")\n",
    "                                continue\n",
    "                            print(f\"Downloading {href}\")\n",
    "                            file_resp = session.get(file_url, stream=True)\n",
    "                            if file_resp.status_code == 200:\n",
    "                                with open(out_path, 'wb') as f:\n",
    "                                    for chunk in file_resp.iter_content(chunk_size=8192):\n",
    "                                        f.write(chunk)\n",
    "                            else:\n",
    "                                print(f\"Failed to download: {href}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error on {year}-{month}-{day}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72bd2c-b901-4f90-9570-bf33dd50b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270ba06-915d-41b8-acf6-da59c5f89761",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.UseExceptions()  # Enable detailed error messages\n",
    "\n",
    "# Define function to extract LST_Day_1km subdataset path from HDF\n",
    "def extract_lst_subdataset(hdf_file):\n",
    "    dataset = gdal.Open(hdf_file)\n",
    "    subdatasets = dataset.GetSubDatasets()\n",
    "    for subdataset in subdatasets:\n",
    "        if 'LST_Day_1km' in subdataset[0]:\n",
    "            return subdataset[0]  # Return the subdataset path\n",
    "    return None\n",
    "\n",
    "# List all HDF files\n",
    "hdf_files = sorted(glob.glob(\"MODIS_MOD11A2/*.hdf\"))\n",
    "\n",
    "# Output folder for GeoTIFFs\n",
    "os.makedirs(\"Extracted_LST_TIFFs\", exist_ok=True)\n",
    "\n",
    "# Loop through HDFs and extract LST\n",
    "for hdf_path in hdf_files:\n",
    "    lst_subdataset = extract_lst_subdataset(hdf_path)\n",
    "    if lst_subdataset:\n",
    "        output_name = os.path.basename(hdf_path).replace(\".hdf\", \"_LST.tif\")\n",
    "        output_path = os.path.join(\"Extracted_LST_TIFFs\", output_name)\n",
    "        if not os.path.exists(output_path):\n",
    "            print(f\"Extracting {output_name}\")\n",
    "            gdal.Translate(output_path, lst_subdataset)\n",
    "        else:\n",
    "            print(f\"Already exists: {output_name}\")\n",
    "    else:\n",
    "        print(f\"LST subdataset not found in {hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc6369-533f-4615-a268-dfdacef0b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale LST Data (Kelvin → Celsius)\n",
    "import rasterio\n",
    "from rasterio import shutil as rio_shutil\n",
    "import numpy as np\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = \"Extracted_LST_TIFFs\"\n",
    "output_dir = \"Scaled_LST_Celsius\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each GeoTIFF\n",
    "for tif_file in sorted(os.listdir(input_dir)):\n",
    "    if tif_file.endswith(\".tif\"):\n",
    "        input_path = os.path.join(input_dir, tif_file)\n",
    "        output_path = os.path.join(output_dir, tif_file.replace(\"_LST.tif\", \"_LST_Celsius.tif\"))\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"Already scaled: {output_path}\")\n",
    "            continue\n",
    "\n",
    "        with rasterio.open(input_path) as src:\n",
    "            profile = src.profile\n",
    "            data = src.read(1).astype(float)\n",
    "\n",
    "            # Handle MODIS fill value\n",
    "            data[data == 0] = np.nan\n",
    "\n",
    "            # Apply scale factor and convert to Celsius\n",
    "            data_celsius = (data * 0.02) - 273.15\n",
    "\n",
    "            profile.update(dtype=rasterio.float32, nodata=np.nan)\n",
    "\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(data_celsius.astype(np.float32), 1)\n",
    "\n",
    "        print(f\"Scaled and saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb52b9-b90b-48dd-aff9-87a1d90c4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Monthly Mean Composites\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Directories\n",
    "input_dir = \"Scaled_LST_Celsius\"\n",
    "output_dir = \"Monthly_Mean_LST\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Group files by year and month\n",
    "file_groups = defaultdict(list)\n",
    "\n",
    "for tif_file in sorted(os.listdir(input_dir)):\n",
    "    if tif_file.endswith(\".tif\"):\n",
    "        parts = tif_file.split(\".\")\n",
    "        date_str = parts[1]  # e.g., A2023145 → year = 2023, doy = 145\n",
    "        year = int(date_str[1:5])\n",
    "        doy = int(date_str[5:])\n",
    "\n",
    "        # Convert DOY to month\n",
    "        dt = datetime(year, 1, 1) + pd.to_timedelta(doy - 1, unit=\"d\")\n",
    "        month = dt.month\n",
    "\n",
    "        if month in [5, 6, 7, 8]:\n",
    "            key = f\"{year}_{month:02d}\"\n",
    "            file_groups[key].append(os.path.join(input_dir, tif_file))\n",
    "\n",
    "# Create mean composites\n",
    "for key, paths in file_groups.items():\n",
    "    output_path = os.path.join(output_dir, f\"LST_Mean_{key}.tif\")\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Already created: {output_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Creating composite: {output_path}\")\n",
    "    arrays = []\n",
    "    profile = None\n",
    "\n",
    "    for path in paths:\n",
    "        with rasterio.open(path) as src:\n",
    "            arr = src.read(1)\n",
    "            arr[arr == src.nodata] = np.nan  # Replace nodata with NaN\n",
    "            arrays.append(arr)\n",
    "            if profile is None:\n",
    "                profile = src.profile\n",
    "\n",
    "    # Compute pixel-wise mean\n",
    "    mean_array = np.nanmean(arrays, axis=0)\n",
    "\n",
    "    profile.update(dtype=rasterio.float32, nodata=np.nan)\n",
    "\n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(mean_array.astype(np.float32), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423091e0-536d-42c2-b6c5-798505f5f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series plotting\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory where monthly means are stored\n",
    "mean_dir = \"Monthly_Mean_LST\"\n",
    "\n",
    "# Collect all files and extract time info\n",
    "mean_data = []\n",
    "\n",
    "for tif_file in sorted(os.listdir(mean_dir)):\n",
    "    if tif_file.endswith(\".tif\"):\n",
    "        parts = tif_file.split(\"_\")  # e.g., LST_Mean_2020_06.tif\n",
    "        year = int(parts[2])\n",
    "        month = int(parts[3].split(\".\")[0])\n",
    "        path = os.path.join(mean_dir, tif_file)\n",
    "\n",
    "        with rasterio.open(path) as src:\n",
    "            arr = src.read(1)\n",
    "            arr[arr == src.nodata] = np.nan\n",
    "            mean_temp = np.nanmean(arr)\n",
    "\n",
    "        mean_data.append((datetime(year, month, 1), mean_temp))\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame(mean_data, columns=[\"Date\", \"Mean_LST\"])\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "for month in [5, 6, 7, 8]:\n",
    "    month_df = df[df[\"Date\"].dt.month == month]\n",
    "    plt.plot(month_df[\"Date\"], month_df[\"Mean_LST\"], marker='o', label=f\"Month {month}\")\n",
    "\n",
    "plt.title(\"Monthly Average LST (MOD11A2) Over South Asia (2000–2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"LST (°C)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed9df1-c2aa-4328-ab7c-5e52892b20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mosaic the extracted files\n",
    "from osgeo import gdal\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"Mosaicked_LST_TIFFs\", exist_ok=True)\n",
    "\n",
    "# Group files by date based on filename pattern\n",
    "file_groups = defaultdict(list)\n",
    "for tif_path in glob.glob(\"Extracted_LST_TIFFs/*_LST.tif\"):\n",
    "    # Extract date string (e.g., A2020153) from filename\n",
    "    base = os.path.basename(tif_path)\n",
    "    date_str = base.split('.')[1]  # e.g., 'A2020153'\n",
    "    file_groups[date_str].append(tif_path)\n",
    "\n",
    "# Mosaic each group\n",
    "for date_str, files in file_groups.items():\n",
    "    output_path = f\"Mosaicked_LST_TIFFs/MOD11A2_{date_str}_mosaic.tif\"\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Already exists: {output_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Mosaicking {date_str} from {len(files)} tiles...\")\n",
    "\n",
    "    # Create virtual raster (VRT)\n",
    "    vrt_path = f\"/vsimem/{date_str}.vrt\"\n",
    "    gdal.BuildVRT(vrt_path, files)\n",
    "\n",
    "    # Translate VRT to GeoTIFF\n",
    "    gdal.Translate(output_path, vrt_path)\n",
    "    gdal.Unlink(vrt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8ef6-f918-4884-93ff-24c63f9fe2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Select one mosaicked LST TIFF\n",
    "tif_path = sorted(glob.glob(\"Mosaicked_LST_TIFFs/*.tif\"))[5]  # Change index to view others\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    lst_data = src.read(1)\n",
    "    lst_data = np.where(lst_data == 0, np.nan, lst_data)  # Handle fill values\n",
    "    lst_data_c = (lst_data * 0.02) - 273.15  # Scale and convert to Celsius\n",
    "    bounds = src.bounds\n",
    "    extent = [bounds.left, bounds.right, bounds.bottom, bounds.top]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.inferno  # or 'hot', 'plasma', etc.\n",
    "plt.imshow(lst_data_c, cmap=cmap, extent=extent)\n",
    "plt.colorbar(label=\"LST (°C)\")\n",
    "plt.title(f\"Mosaicked LST: {os.path.basename(tif_path)}\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a640204-00d3-46b6-a9d8-a7ee7e987fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholding high temp areas > 40 degress\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Threshold temperature in Kelvin (e.g., 40°C = 313.15K)\n",
    "TEMP_THRESHOLD = 313.15\n",
    "\n",
    "# Output folder for thresholded binary rasters\n",
    "os.makedirs(\"Thresholded_Rasters\", exist_ok=True)\n",
    "\n",
    "# List clipped LST GeoTIFFs\n",
    "clipped_files = sorted(glob.glob(\"Mosaicked_LST_TIFFs/*.tif\"))\n",
    "\n",
    "for tif_path in clipped_files:\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        lst = src.read(1).astype(np.float32)\n",
    "        profile = src.profile\n",
    "\n",
    "        # MODIS LST scaling factor (0.02) and mask for no data\n",
    "        lst[lst == 0] = np.nan\n",
    "        lst = lst * 0.02\n",
    "\n",
    "        # Create binary mask: 1 where LST > threshold, else 0\n",
    "        binary = np.where(lst > TEMP_THRESHOLD, 1, 0).astype(np.uint8)\n",
    "\n",
    "        # Update profile to uint8 and remove scale info\n",
    "        profile.update(dtype=rasterio.uint8, count=1, nodata=0)\n",
    "\n",
    "        output_path = os.path.join(\"Thresholded_Rasters\", os.path.basename(tif_path).replace(\"_clipped.tif\", \"_thresholded.tif\"))\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(binary, 1)\n",
    "\n",
    "        print(f\"Thresholded: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6439bf-7ab9-4488-bef5-fd395faec5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Thresholded Rasters into Shapefiles\n",
    "import os\n",
    "import glob\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"Vectorized_UHI_Shapes\", exist_ok=True)\n",
    "\n",
    "for tif_path in sorted(glob.glob(\"Thresholded_Rasters/*.tif\")):\n",
    "    output_name = os.path.basename(tif_path).replace(\".tif\", \".shp\")\n",
    "    output_path = os.path.join(\"Vectorized_UHI_Shapes\", output_name)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        image = src.read(1)\n",
    "        mask = image == 1  # Only vectorize high-temp pixels\n",
    "\n",
    "        print(f\"Vectorizing {output_name}...\")\n",
    "        results = list(\n",
    "            {\"geometry\": shape(geom), \"properties\": {\"value\": value}}\n",
    "            for geom, value in shapes(image, mask=mask, transform=src.transform)\n",
    "        )\n",
    "\n",
    "        if not results:\n",
    "            print(f\"⚠️  No high-temp areas found in: {output_name}\")\n",
    "            continue  # Skip writing empty shapefile\n",
    "\n",
    "        gdf = gpd.GeoDataFrame.from_features(results, crs=src.crs)\n",
    "        gdf.to_file(output_path)\n",
    "\n",
    "print(\"✅ Vectorization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802454d4-b34a-4a73-8d4d-8b3c3fdb4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group vector files by year for visualization\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "vector_dir = \"Vectorized_UHI_Shapes\"\n",
    "output_dir = \"Yearly_UHI_Shapes\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Collect files by year\n",
    "year_groups = defaultdict(list)\n",
    "for shp_path in glob.glob(os.path.join(vector_dir, \"*.shp\")):\n",
    "    fname = os.path.basename(shp_path)\n",
    "    if \"A\" in fname:\n",
    "        year = fname.split(\"_A\")[1][:4]\n",
    "        year_groups[year].append(shp_path)\n",
    "\n",
    "# Merge and export one GeoPackage per year\n",
    "for year, paths in year_groups.items():\n",
    "    gdf_list = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            gdf = gpd.read_file(path)\n",
    "            if not gdf.empty:\n",
    "                gdf_list.append(gdf)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "    \n",
    "    if gdf_list:\n",
    "        merged = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "        merged.to_file(os.path.join(output_dir, f\"UHI_{year}.gpkg\"), driver=\"GPKG\")\n",
    "        print(f\"✅ Saved UHI_{year}.gpkg\")\n",
    "    else:\n",
    "        print(f\"⚠️ No data for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a01012-bbe6-4dc3-8b12-1562d8569f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize yearly changes\n",
    "import folium\n",
    "from ipywidgets import interact, widgets\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Path to yearly GPKGs\n",
    "yearly_gpkg_dir = \"Yearly_UHI_Shapes\"\n",
    "years = sorted([f.split(\"_\")[1].split(\".\")[0] for f in os.listdir(yearly_gpkg_dir) if f.endswith(\".gpkg\")])\n",
    "\n",
    "def show_uhi(year):\n",
    "    gpkg_path = os.path.join(yearly_gpkg_dir, f\"UHI_{year}.gpkg\")\n",
    "    \n",
    "    try:\n",
    "        gdf = gpd.read_file(gpkg_path).to_crs(epsg=4326)\n",
    "\n",
    "        # Create map centered on bounds of the GDF\n",
    "        bounds = gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "        center = [(bounds[1] + bounds[3]) / 2, (bounds[0] + bounds[2]) / 2]\n",
    "        m = folium.Map(location=center, zoom_start=6, tiles='CartoDB positron')\n",
    "\n",
    "        # Optional: add area in sq.km if not already present\n",
    "        if 'area_km2' not in gdf.columns:\n",
    "            gdf = gdf.to_crs(epsg=3857)\n",
    "            gdf['area_km2'] = gdf.geometry.area / 1e6\n",
    "            gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "        # Style and tooltip\n",
    "        folium.GeoJson(\n",
    "            gdf,\n",
    "            style_function=lambda x: {\n",
    "                'fillColor': '#FF5733',\n",
    "                'color': '#C70039',\n",
    "                'weight': 1,\n",
    "                'fillOpacity': 0.5\n",
    "            },\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=['area_km2'],\n",
    "                aliases=['Area (km²):'],\n",
    "                localize=True\n",
    "            ),\n",
    "            name=f\"Urban Heat Islands - {year}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "        folium.LayerControl(collapsed=False).add_to(m)\n",
    "        return m\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {year}: {e}\")\n",
    "        return folium.Map(location=[30.3753, 69.3451], zoom_start=5, tiles='CartoDB positron')\n",
    "\n",
    "# Interactive Dropdown\n",
    "interact(show_uhi, year=widgets.Dropdown(options=years, description='Year:'));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8287acf-bb14-4ded-bc59-8b915836f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "# Load and reproject to a projected CRS (for centroid accuracy)\n",
    "gdf = gpd.read_file(\"Yearly_UHI_Shapes/UHI_2024.gpkg\")\n",
    "gdf_proj = gdf.to_crs(epsg=3857)\n",
    "gdf_proj['centroid'] = gdf_proj.geometry.centroid\n",
    "\n",
    "# Convert centroids back to WGS84\n",
    "gdf['centroid'] = gdf_proj['centroid'].to_crs(epsg=4326)\n",
    "\n",
    "# Create heatmap data\n",
    "heat_data = [[pt.y, pt.x] for pt in gdf['centroid'].geometry]\n",
    "\n",
    "# Use string keys to avoid float interpretation error\n",
    "custom_gradient = {\n",
    "    \"0.2\": 'orange',\n",
    "    \"0.5\": 'yellow',\n",
    "    \"0.8\": 'red'\n",
    "}\n",
    "\n",
    "# Create map\n",
    "m = folium.Map(location=[30.3753, 69.3451], zoom_start=5, tiles='CartoDB positron')\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    radius=8,\n",
    "    blur=12,\n",
    "    min_opacity=0.2,\n",
    "    gradient=custom_gradient\n",
    ").add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18885522-38b8-4c15-a313-f9fcb3fead5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import geopandas as gpd\n",
    "from ipywidgets import interact, widgets\n",
    "import os\n",
    "\n",
    "# Folder containing yearly GPKG files\n",
    "yearly_gpkg_dir = \"Yearly_UHI_Shapes\"\n",
    "years = sorted([f.split(\"_\")[1].split(\".\")[0] for f in os.listdir(yearly_gpkg_dir) if f.endswith(\".gpkg\")])\n",
    "\n",
    "# Center map on Pakistan\n",
    "m_center = [30.3753, 69.3451]  # Rough center\n",
    "\n",
    "# Define custom gradient\n",
    "custom_gradient = {\n",
    "    \"0.2\": 'orange',\n",
    "    \"0.5\": 'yellow',\n",
    "    \"0.8\": 'red'\n",
    "}\n",
    "\n",
    "def show_uhi_heatmap(year):\n",
    "    # Load and reproject to projected CRS for accurate centroid calculation\n",
    "    gpkg_path = os.path.join(yearly_gpkg_dir, f\"UHI_{year}.gpkg\")\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "    \n",
    "    if gdf.empty:\n",
    "        print(f\"No data for year {year}\")\n",
    "        return folium.Map(location=m_center, zoom_start=5, tiles='CartoDB positron')\n",
    "\n",
    "    gdf_proj = gdf.to_crs(epsg=3857)\n",
    "    gdf_proj['centroid'] = gdf_proj.geometry.centroid\n",
    "\n",
    "    # Convert centroids back to WGS84\n",
    "    gdf['centroid'] = gdf_proj['centroid'].to_crs(epsg=4326)\n",
    "\n",
    "    # Create heatmap data\n",
    "    heat_data = [[pt.y, pt.x] for pt in gdf['centroid'].geometry]\n",
    "\n",
    "    # Create folium map\n",
    "    m = folium.Map(location=m_center, zoom_start=5, tiles='CartoDB positron')\n",
    "    HeatMap(\n",
    "        heat_data,\n",
    "        radius=8,\n",
    "        blur=12,\n",
    "        min_opacity=0.2,\n",
    "        gradient=custom_gradient\n",
    "    ).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Create interactive widget\n",
    "interact(show_uhi_heatmap, year=widgets.Dropdown(options=years, description='Year:'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f589ae-6088-4525-9e96-e4a04f7c097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load thresholded binary UHI rasters\n",
    "thresholded_dir = \"Thresholded_Rasters\"\n",
    "uhi_files = sorted(glob.glob(os.path.join(thresholded_dir, \"*.tif\")))\n",
    "uhi_stack = []\n",
    "\n",
    "for f in uhi_files:\n",
    "    with rasterio.open(f) as src:\n",
    "        uhi_stack.append(src.read(1))\n",
    "\n",
    "uhi_stack = np.stack(uhi_stack)\n",
    "total_years = uhi_stack.shape[0]\n",
    "\n",
    "# Calculate UHI frequency map\n",
    "uhi_occurrence = np.sum(uhi_stack, axis=0)\n",
    "\n",
    "# Set thresholds\n",
    "min_valid_years = 3  # ignore < 3 occurrences (noise)\n",
    "persistent_thresh = int(0.6 * total_years)  # ~60% of years\n",
    "\n",
    "# Apply classification\n",
    "persistent_mask = (uhi_occurrence >= persistent_thresh).astype(np.uint8)\n",
    "transient_mask = ((uhi_occurrence >= min_valid_years) & \n",
    "                  (uhi_occurrence < persistent_thresh)).astype(np.uint8)\n",
    "\n",
    "# Save results\n",
    "output_dir = \"UHI_Persistence_Maps\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with rasterio.open(uhi_files[0]) as src:\n",
    "    meta = src.meta.copy()\n",
    "meta.update(dtype='uint8', count=1)\n",
    "\n",
    "with rasterio.open(os.path.join(output_dir, \"Persistent_UHI.tif\"), 'w', **meta) as dst:\n",
    "    dst.write(persistent_mask, 1)\n",
    "\n",
    "with rasterio.open(os.path.join(output_dir, \"Transient_UHI.tif\"), 'w', **meta) as dst:\n",
    "    dst.write(transient_mask, 1)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(persistent_mask, cmap='Reds')\n",
    "plt.title(\"Persistent UHI (≥ 60% of years)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(transient_mask, cmap='Oranges')\n",
    "plt.title(\"Transient UHI (3–60% of years)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef256e3-804b-411b-acb4-5686adbde48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually dowloaded pdf osm files from geofabrick\n",
    "# conversion from pbf extension to gpkg format\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set this to the folder containing your .pbf files\n",
    "pbf_dir = \"OSM_Geofabrik\"\n",
    "gpkg_dir = \"Geofabrik_GPKG\"\n",
    "os.makedirs(gpkg_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all PBF files in the folder\n",
    "for fname in os.listdir(pbf_dir):\n",
    "    if fname.endswith(\".osm.pbf\"):\n",
    "        country_name = fname.replace(\"-latest.osm.pbf\", \"\")\n",
    "        pbf_path = os.path.join(pbf_dir, fname)\n",
    "        gpkg_path = os.path.join(gpkg_dir, f\"{country_name}.gpkg\")\n",
    "        \n",
    "        if os.path.exists(gpkg_path):\n",
    "            print(f\"✓ Skipping (already exists): {gpkg_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"🚀 Converting: {fname} → {gpkg_path}\")\n",
    "            cmd = f'ogr2ogr -f \"GPKG\" \"{gpkg_path}\" \"{pbf_path}\"'\n",
    "            subprocess.run(cmd, shell=True, check=True)\n",
    "            print(f\"✅ Done: {gpkg_path}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Error converting {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315481ea-d859-41bf-bd5a-60a93228ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Urban Features from GPKG Files\n",
    "import os\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Input: directory of GPKG files\n",
    "gpkg_dir = \"Geofabrik_GPKG\"\n",
    "output_dir = \"Urban_GPKG_Extracts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Tags of interest for urban classification\n",
    "urban_keywords = ['city', 'town', 'suburb', 'residential', 'commercial', 'industrial']\n",
    "\n",
    "for fname in os.listdir(gpkg_dir):\n",
    "    if fname.endswith(\".gpkg\"):\n",
    "        country = fname.replace(\".gpkg\", \"\")\n",
    "        gpkg_path = os.path.join(gpkg_dir, fname)\n",
    "        output_path = os.path.join(output_dir, f\"urban_{country}.gpkg\")\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"✓ Skipping (already processed): {output_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"🔍 Processing {country}...\")\n",
    "\n",
    "        try:\n",
    "            # List layers in the GPKG\n",
    "            layers = fiona.listlayers(gpkg_path)\n",
    "            \n",
    "            urban_features = []\n",
    "\n",
    "            for layer in layers:\n",
    "                gdf = gpd.read_file(gpkg_path, layer=layer)\n",
    "\n",
    "                # Filter for columns with 'place' or 'landuse' or any descriptive tags\n",
    "                for col in gdf.columns:\n",
    "                    if gdf[col].dtype == object:\n",
    "                        gdf[col] = gdf[col].fillna(\"\").astype(str)\n",
    "                        match = gdf[gdf[col].str.lower().isin(urban_keywords)]\n",
    "                        if not match.empty:\n",
    "                            urban_features.append(match)\n",
    "\n",
    "            if urban_features:\n",
    "                urban_gdf = pd.concat(urban_features, ignore_index=True)\n",
    "                urban_gdf = gpd.GeoDataFrame(urban_gdf, crs=\"EPSG:4326\")\n",
    "                urban_gdf = urban_gdf[urban_gdf.geometry.notnull() & ~urban_gdf.geometry.is_empty]\n",
    "                urban_gdf.to_file(output_path, driver=\"GPKG\")\n",
    "                print(f\"✅ Urban features saved: {output_path}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No urban features found for {country}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {country}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec6889-3e4f-44d2-aad3-4eb55add7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify UHI Zones Using Urban GPKG Overlays\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "uhi_dir = \"Yearly_UHI_Shapes\"\n",
    "urban_dir = \"Urban_GPKG_Extracts\"\n",
    "output_dir = \"UHI_UrbanRural_Classified\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and combine urban features\n",
    "urban_gdfs = []\n",
    "for fname in os.listdir(urban_dir):\n",
    "    if fname.endswith(\".gpkg\"):\n",
    "        print(f\"Reading urban data: {fname}\")\n",
    "        gdf = gpd.read_file(os.path.join(urban_dir, fname)).to_crs(\"EPSG:4326\")\n",
    "        urban_gdfs.append(gdf)\n",
    "\n",
    "urban_all = gpd.GeoDataFrame(pd.concat(urban_gdfs, ignore_index=True), crs=\"EPSG:4326\")\n",
    "urban_all = urban_all[urban_all.geometry.notnull() & ~urban_all.geometry.is_empty]\n",
    "\n",
    "# Process each UHI shapefile\n",
    "for fname in sorted(os.listdir(uhi_dir)):\n",
    "    if fname.endswith(\".gpkg\"):\n",
    "        year = fname.split(\"_\")[1].split(\".\")[0]\n",
    "        print(f\"Classifying {year}...\")\n",
    "\n",
    "        uhi_path = os.path.join(uhi_dir, fname)\n",
    "        uhi_gdf = gpd.read_file(uhi_path).to_crs(\"EPSG:4326\")\n",
    "        uhi_gdf = uhi_gdf.reset_index(drop=True)\n",
    "\n",
    "        # Add an explicit ID to preserve indexing\n",
    "        uhi_gdf[\"UHI_ID\"] = uhi_gdf.index\n",
    "\n",
    "        # Spatial join\n",
    "        joined = gpd.sjoin(uhi_gdf, urban_all, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "        # Get unique UHI_IDs that intersect urban areas\n",
    "        urban_matches = joined[\"UHI_ID\"].drop_duplicates()\n",
    "\n",
    "        # Assign Rural by default, Urban where matched\n",
    "        uhi_gdf[\"UHI_Type\"] = \"Rural\"\n",
    "        uhi_gdf.loc[uhi_gdf[\"UHI_ID\"].isin(urban_matches), \"UHI_Type\"] = \"Urban\"\n",
    "\n",
    "        # Save output\n",
    "        out_path = os.path.join(output_dir, f\"UHI_{year}_classified.gpkg\")\n",
    "        uhi_gdf.drop(columns=[\"UHI_ID\"]).to_file(out_path, driver=\"GPKG\")\n",
    "        print(f\"✔ Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c64e3-3cd9-49f5-a02c-46b1898f8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  UHI Urban vs Rural Summary Dashboard\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set paths\n",
    "classified_dir = \"UHI_UrbanRural_Classified\"\n",
    "\n",
    "# Collect data for all years\n",
    "summary_data = []\n",
    "\n",
    "for fname in sorted(os.listdir(classified_dir)):\n",
    "    if fname.endswith(\".gpkg\"):\n",
    "        year = fname.split(\"_\")[1]\n",
    "        path = os.path.join(classified_dir, fname)\n",
    "        \n",
    "        gdf = gpd.read_file(path)\n",
    "        counts = gdf[\"UHI_Type\"].value_counts().to_dict()\n",
    "        \n",
    "        summary_data.append({\n",
    "            \"Year\": int(year),\n",
    "            \"Urban\": counts.get(\"Urban\", 0),\n",
    "            \"Rural\": counts.get(\"Rural\", 0),\n",
    "            \"Total\": sum(counts.values())\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "summary_df = pd.DataFrame(summary_data).sort_values(\"Year\")\n",
    "summary_df.set_index(\"Year\", inplace=True)\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(12, 6))\n",
    "summary_df[[\"Urban\", \"Rural\"]].plot(kind='bar', stacked=True, colormap=\"Set2\")\n",
    "plt.title(\"Urban vs Rural UHI Counts per Year\")\n",
    "plt.ylabel(\"Number of UHI Zones\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save summary as CSV\n",
    "summary_df.to_csv(\"UHI_UrbanRural_Summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff07a1-e042-45eb-9e32-959404bd02a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
